{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping with Python\n",
    "\n",
    "### you will learn the basics of how to extract data from websites and visualize it using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install packages\n",
    "\n",
    "#### open a new terminal and install 'requests', 'beautiful soup' and 'pandas' by typing:\n",
    "#### conda install -c anaconda requests\n",
    "#### conda install -c anaconda beautifulsoup4 \n",
    "#### conda install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import __version__ as bs4__version__ \n",
    "'''\n",
    "The Beautiful Soup package is used to parse the html, that is, take the raw html text and break it into Python objects\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np # LET'S START WITH NUMPY!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('requets: ' + requests.__version__, 'bs4: ' + bs4__version__,'pandas: ' + pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://www.federalreserve.gov/apps/fof/DisplayTable.aspx?t=f.105'\n",
    "url = 'https://coinmarketcap.com/all/views/all/'\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res  = requests.get(url)\n",
    "soup = BeautifulSoup(res.content, 'lxml') #The second argument 'lxml' is the html parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can use the find_all() method of soup to extract useful html tags within a webpage. Examples of useful tags include < a > for hyperlinks, < table > for tables, < tr > for table rows, < th > for table headers, and < td > for table cells. The code below shows how to extract all the hyperlinks within the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find_all('table')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = soup.find_all('table')[0] \n",
    "df    = pd.read_html(str(table))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['% 7d'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['% 7d']   = df['% 7d'].map(lambda x: x.strip('?%')).apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['% 7d'].loc[:20].plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['% 7d'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework - find the highest, lowest and average price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r    = requests.get(\"http://newyork.craigslist.org/search/aap\")\n",
    "r.raise_for_status()\n",
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_spans = soup.select(\"span.result-price\")\n",
    "prices      = [int(span.text[1:]) for span in price_spans]\n",
    " \n",
    "print('Highest price: ${}'.format(max(prices)))\n",
    "print('Lowest price: ${}'.format(min(prices)))\n",
    "print('Average price: ${}'.format(sum(prices)/len(prices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_url = 'https://www.alibaba.com/trade/search?fsb=y&IndexArea=product_en&CatId=&SearchText='\n",
    "query    = 'microcontroller'\n",
    "\n",
    "r    = requests.get(base_url + query)\n",
    "r.raise_for_status()\n",
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_spans = soup.find_all(\"div\", attrs={\"price\"})\n",
    "print(len(price_spans))\n",
    "\n",
    "price_mins, price_maxs = [], []\n",
    "for span in price_spans:\n",
    "    min_max_price = [float(x) for x in re.findall('\\$(\\d+\\.\\d+)', span.text)]\n",
    "    if len(min_max_price)==1:\n",
    "        min_max_price.append(np.nan)\n",
    "    price_mins.append( min_max_price[0] )\n",
    "    price_maxs.append( min_max_price[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "min_spans = soup.find_all(\"div\", attrs={\"min-order\"})\n",
    "print(len(min_spans))\n",
    "\n",
    "min_order = []\n",
    "for span in min_spans:\n",
    "    min_order.append( re.findall('\\d+', span.text)[0]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name_spans = soup.find_all(\"h2\", attrs={\"title\"})\n",
    "print(len(name_spans))\n",
    "\n",
    "company_names = []\n",
    "for name in name_spans:\n",
    "    company_names.append(name.text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "href_spans = soup.find_all(\"div\", attrs={\"stitle util-ellipsis\"})\n",
    "print(len(href_spans))\n",
    "\n",
    "company_hrefs = []\n",
    "for href in href_spans:\n",
    "    company_hrefs.append( re.findall( 'href=\"([^\"]*)\"', str(href.find_all('a')) )[0].strip() ) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.array([company_names, company_hrefs, min_order, price_mins, price_maxs]).T,\n",
    "             columns=['product_details','company_site','moq','price_min','price_max'])\n",
    "\n",
    "for k, v in df.iteritems():\n",
    "    if str(k) in ['moq','price_min','price_max']:\n",
    "        df[k] = pd.to_numeric(df[k], errors='coerce')\n",
    "        \n",
    "def make_clickable(val):\n",
    "    # target _blank to open new window\n",
    "    return '<a target=\"_blank\" href=\"{}\">{}</a>'.format(val, val)\n",
    "\n",
    "df.style.format({'company_site': make_clickable})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.price_max.plot()\n",
    "df.price_min.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
